{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 带有一维分类器的循环一致生成对抗网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取样本数据和标签数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_signals = np.load('simulate_signals.npy')  \n",
    "target_signals = np.load('real_signals.npy') \n",
    "source_labels = np.load('simulate_labels.npy')\n",
    "target_labels = np.load('real_labels.npy')\n",
    "\n",
    "print(source_signals.shape)\n",
    "print(target_signals.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建生成器、判别器和分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "tf.random.set_seed(seed)\n",
    "initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "class residual_block(layers.Layer):         #自定义残差块\n",
    "    def __init__(self, filters=8, kernel_size=12, strides=1, **kwargs):\n",
    "        super(residual_block, self).__init__(**kwargs)\n",
    "        self.conv = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer=initializer)\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.relu = layers.ReLU()\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = layers.add([inputs, x])\n",
    "        return self.relu(x)\n",
    "\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=(2048,1)))\n",
    "    model.add(layers.Conv1D(filters=20, kernel_size=64, strides=8, padding='same', kernel_initializer=initializer))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Conv1D(filters=30, kernel_size=24, strides=4, padding='same', kernel_initializer=initializer))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Conv1D(filters=40, kernel_size=12, strides=4, padding='same', kernel_initializer=initializer))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(residual_block(filters=40, kernel_size=12, strides=1))\n",
    "    model.add(residual_block(filters=40, kernel_size=12, strides=1))\n",
    "    model.add(residual_block(filters=40, kernel_size=12, strides=1))\n",
    "    model.add(layers.Conv1DTranspose(filters=30, kernel_size=12, strides=4, padding='same', kernel_initializer=initializer))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Conv1DTranspose(filters=20, kernel_size=24, strides=4, padding='same', kernel_initializer=initializer))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Conv1DTranspose(filters=1, kernel_size=64, strides=8, padding='same', activation='tanh', kernel_initializer=initializer))\n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=(2048,1)))\n",
    "    model.add(layers.Conv1D(filters=20, kernel_size=64, strides=8, padding='same', kernel_initializer=initializer))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Conv1D(filters=30, kernel_size=24, strides=4, padding='same', kernel_initializer=initializer))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Conv1D(filters=40, kernel_size=12, strides=4, padding='same', kernel_initializer=initializer))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation='sigmoid', kernel_initializer=initializer))\n",
    "    return model\n",
    "\n",
    "def build_classifier(num_class=4):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=(2048,1)))\n",
    "    model.add(layers.Conv1D(filters=20, kernel_size=64, strides=8, padding='same', kernel_initializer=initializer))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Conv1D(filters=30, kernel_size=24, strides=4, padding='same', kernel_initializer=initializer))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(500, kernel_initializer=initializer))\n",
    "    model.add(layers.Dense(num_class, activation='softmax', kernel_initializer=initializer))\n",
    "    return model\n",
    "\n",
    "# generator = build_generator()\n",
    "# discriminator = build_discriminator()\n",
    "# classifier = build_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    real_loss = loss_object(tf.ones_like(real), real)\n",
    "    fake_loss = loss_object(tf.zeros_like(fake), fake)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(fake):\n",
    "    return loss_object(tf.ones_like(fake), fake)\n",
    "\n",
    "def cycle_loss(real, cycled):\n",
    "    return tf.reduce_mean(tf.abs(tf.cast(real, tf.float64) - tf.cast(cycled, tf.float64)))\n",
    "\n",
    "def classifier_loss(real, pred):\n",
    "    return tf.cast(loss_object(real, pred), tf.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本超参数\n",
    "uncorresponded_weight = 0.3\n",
    "cycle_loss_weight = 10\n",
    "source_batch = 16\n",
    "target_batch = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义训练流程和关键模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_G = build_generator()  # 从源域到目标域\n",
    "generator_F = build_generator()  # 从目标域到源域\n",
    "discriminator_Ds = build_discriminator()  # 判别源域\n",
    "discriminator_Dt = build_discriminator()  # 判别目标域\n",
    "classifier_Cs = build_classifier()  # 对源域信号进行分类\n",
    "classifier_Ct = build_classifier()  # 对目标域信号进行分类\n",
    "\n",
    "generator_G_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002, beta_1=0.5)\n",
    "generator_F_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002, beta_1=0.5)\n",
    "discriminator_Ds_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002, beta_1=0.5)\n",
    "discriminator_Dt_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002, beta_1=0.5)\n",
    "classifier_Cs_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "classifier_Ct_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "# @tf.function\n",
    "def train_on_batch(real_x, real_y, real_label_x, real_label_y, normal_count):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        fake_y = generator_G(real_x)\n",
    "        cycled_x = generator_F(fake_y)\n",
    "\n",
    "        fake_x = generator_F(real_y)\n",
    "        cycled_y = generator_G(fake_x)\n",
    "\n",
    "        # 判别器结果\n",
    "        disc_real_x = discriminator_Ds(real_x)\n",
    "        disc_fake_x = discriminator_Ds(fake_x)\n",
    "\n",
    "        disc_real_y = discriminator_Dt(real_y)\n",
    "        disc_fake_y = discriminator_Dt(fake_y)\n",
    "\n",
    "        # 分类器结果\n",
    "        pred_labels_real_x = classifier_Cs(real_x)\n",
    "        pred_labels_fake_x = classifier_Cs(fake_x)\n",
    "        \n",
    "        pred_labels_real_y = classifier_Ct(real_y)\n",
    "        pred_labels_fake_y = classifier_Ct(fake_y)\n",
    "\n",
    "        # 判别器损失\n",
    "        disc_Ds_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
    "        disc_Dt_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
    "\n",
    "        # 分类器损失\n",
    "        class_Cs_loss = classifier_loss(real_label_x, pred_labels_real_x) + classifier_loss(real_label_y, pred_labels_fake_x)\n",
    "        class_Ct_loss = classifier_loss(real_label_x[:normal_count], pred_labels_fake_y[:normal_count]) + uncorresponded_weight * classifier_loss(real_label_x[normal_count:], pred_labels_fake_y[normal_count:])\n",
    "\n",
    "        # 生成器损失\n",
    "        gen_G_loss = tf.cast(disc_Ds_loss, tf.float64) + tf.cast(disc_Dt_loss, tf.float64) + cycle_loss_weight * (cycle_loss(tf.reshape(real_x, (source_batch,2048,1)), cycled_x) + cycle_loss(tf.reshape(real_y, (target_batch,2048,1)), cycled_y)) + class_Cs_loss + class_Ct_loss\n",
    "        gen_F_loss = tf.cast(disc_Ds_loss, tf.float64) + tf.cast(disc_Dt_loss, tf.float64) + cycle_loss_weight * (cycle_loss(tf.reshape(real_x, (source_batch,2048,1)), cycled_x) + cycle_loss(tf.reshape(real_y, (target_batch,2048,1)), cycled_y)) + class_Cs_loss + class_Ct_loss\n",
    "\n",
    "    print(f'gen_loss: {gen_G_loss}, class_Ct_loss: {class_Ct_loss}, disc_Dt_loss: {disc_Dt_loss}')\n",
    "    \n",
    "    # 计算梯度\n",
    "    gen_G_gradients = tape.gradient(gen_G_loss, generator_G.trainable_variables)\n",
    "    gen_F_gradients = tape.gradient(gen_F_loss, generator_F.trainable_variables)\n",
    "    disc_Ds_gradients = tape.gradient(disc_Ds_loss, discriminator_Ds.trainable_variables)\n",
    "    disc_Dt_gradients = tape.gradient(disc_Dt_loss, discriminator_Dt.trainable_variables)\n",
    "    class_Cs_gradients = tape.gradient(class_Cs_loss, classifier_Cs.trainable_variables)\n",
    "    class_Ct_gradients = tape.gradient(class_Ct_loss, classifier_Ct.trainable_variables)\n",
    "\n",
    "    # 更新权重参数\n",
    "    generator_G_optimizer.apply_gradients(zip(gen_G_gradients, generator_G.trainable_variables))\n",
    "    generator_F_optimizer.apply_gradients(zip(gen_F_gradients, generator_F.trainable_variables))\n",
    "    discriminator_Ds_optimizer.apply_gradients(zip(disc_Ds_gradients, discriminator_Ds.trainable_variables))\n",
    "    discriminator_Dt_optimizer.apply_gradients(zip(disc_Dt_gradients, discriminator_Dt.trainable_variables))\n",
    "    classifier_Cs_optimizer.apply_gradients(zip(class_Cs_gradients, classifier_Cs.trainable_variables))\n",
    "    classifier_Ct_optimizer.apply_gradients(zip(class_Ct_gradients, classifier_Ct.trainable_variables))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 4\n",
    "def get_train_batch(source_signals, target_signals, source_labels, target_labels, source_batch, target_batch):\n",
    "    if (source_batch > len(source_labels)) or (target_batch > len(target_labels)):\n",
    "        print(\"batch size is too large\")\n",
    "        return None, None, None, None, None\n",
    "    selected_source_indices = np.random.choice(len(source_labels), size=source_batch, replace=False)\n",
    "    selected_target_indices = np.random.choice(len(target_labels), size=target_batch, replace=False)\n",
    "\n",
    "    selected_source_indices = np.sort(selected_source_indices)\n",
    "    normal_count = np.sum(selected_source_indices < (len(source_labels) / num_class))     # 这里默认模拟生成的各类别样本数量相同，如果模拟样本数量不一样，则索引号应该小于总样本集中正常样本数量\n",
    "    selected_source_signals = source_signals[selected_source_indices]\n",
    "    selected_source_labels = source_labels[selected_source_indices]\n",
    "    # print(selected_source_labels)\n",
    "    selected_target_signals = target_signals[selected_target_indices]\n",
    "    selected_target_labels = target_labels[selected_target_indices]\n",
    "\n",
    "    return selected_source_signals, selected_source_labels, selected_target_signals, selected_target_labels, normal_count\n",
    "\n",
    "def label_to_oneshot(label, num_classes):\n",
    "    output = np.zeros((len(label), num_classes))\n",
    "    for i in range(len(label)):\n",
    "        index = label[i]\n",
    "        output[i][int(index)] = 1\n",
    "    return output\n",
    "\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    real_x, real_label_x, real_y, real_label_y, normal_count = get_train_batch(source_signals, target_signals, source_labels, target_labels, source_batch, target_batch)\n",
    "    train_on_batch(real_x, real_y, label_to_oneshot(real_label_x, 4), label_to_oneshot(real_label_y, 4), normal_count)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS} completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试用例\n",
    "(整体流程，只是一个能跑的例子，试了一下，效果很差，不过模型基本上是对的，样本和训练需要增加)\n",
    "根据真实和模拟样本数量之间的比值，调整Ct损失函数中uncorresponded_weight的数值，范围（0，1），太大会过拟合，模拟样本数较少的话，可以适当调大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_signal in source_signals:\n",
    "    test_signal = test_signal.reshape(1, 2048, 1)\n",
    "    pred_label = classifier_Ct(generator_G(test_signal))\n",
    "    print(pred_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
